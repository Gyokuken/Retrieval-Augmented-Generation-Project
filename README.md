# Simple RAG System (FastAPI + Groq)

A minimal, production-ready **Retrieval-Augmented Generation (RAG)** backend built with **FastAPI**, **LangChain**, and **Groq LLMs**.  
You ingest documents, embed them, store them in a vector database, and query them using an LLM that actually knows your data instead of hallucinating nonsense.

---

## ğŸš€ Features

- FastAPI backend
- Groq-powered LLM inference (fast + cheap)
- Document ingestion & chunking
- Vector search using embeddings
- Clean RAG pipeline (Retriever â†’ LLM â†’ Answer)
- Easy to extend (frontend, auth, DB swap, etc.)

---

## ğŸ§  Architecture (High Level)

1. Documents are loaded and split into chunks  
2. Chunks are converted to embeddings  
3. Embeddings are stored in a vector store  
4. User query â†’ relevant chunks retrieved  
5. Groq LLM generates an answer using retrieved context  

This is **actual RAG**, not â€œprompt stuffing pretending to be RAGâ€.

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py # FastAPI entry point
â”‚ â”œâ”€â”€ rag.py # RAG pipeline logic
â”‚ â”œâ”€â”€ ingest.py # Document ingestion
â”‚ â”œâ”€â”€ seed.py # Initial data seeding
â”‚ â”œâ”€â”€ config.py # Environment & constants
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ data/ # Documents to ingest
â””â”€â”€ README.md
```


---

## ğŸ› ï¸ Requirements

- Python **3.9+**
- Groq API key
- Virtual environment (recommended unless you enjoy dependency hell)

---

## ğŸ“¦ Installation

```bash
git clone <your-repo-url>
cd backend

python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

pip install -r requirements.txt
```
---

## Environment Variables
Create a .env file inside backend/:
```bash
GROQ_API_KEY=your_groq_api_key_here
```
No key = no LLM = system is useless. Donâ€™t skip this.

Seeding / Ingesting Data
Put your documents inside the data/ folder, then run:
```bash
python seed.py
```
---

This will:

-- Load documents

-- Split them into chunks

-- Generate embeddings

-- Store them in the vector database

-- Run this once, or whenever data changes.

Running the Server
``` bash
uvicorn main:app --reload
```

Server will start at:
```bash
http://127.0.0.1:8000
```

---

Swagger UI (use this, it's there for a reasons)
```bash
http://127.0.0.1:8000/docs
```
## ğŸ” Querying the RAG System

Typical flow:

Send a query to the /query endpoint

Backend retrieves relevant chunks

Groq LLM answers using retrieved context only

If your answers are bad:
 
Your chunks are bad

Your embeddings are bad

Or your documents are trash
(Yes, this happens more often than people admit.)

## requirements.txt (Important)
Groq is included â€” this was missing earlier and that was a legit catch.

```bash
fastapi
uvicorn
python-dotenv
langchain
langchain-community
langchain-groq
chromadb
tiktoken
pydantic
```

If Groq isnâ€™t here, RAG isnâ€™t happening. Period.

---

## ğŸ§ª Notes & Gotchas
RAG quality depends more on chunking strategy than the LLM

Smaller chunks â‰  always better

Donâ€™t blindly increase k in retrieval â€” youâ€™ll just add noise

This backend works fine without a frontend; test via Swagger or Postman

---

## ğŸ§© Next Obvious Improvements
Auth (JWT)

Streaming responses

Hybrid search (BM25 + vectors)

Persistent vector DB (Postgres + pgvector)

Frontend (only after backend is rock solid)

---

## ğŸ§  Final Reality Check

This is a real RAG backend, not a tutorial toy.
If you want hallucination-free answers, this is the baseline â€” not the finish line.

If you want:

--Advanced chunking

-- Evaluation metrics

-- Multi-document reasoning

-- Agentic RAG

Say the word.
